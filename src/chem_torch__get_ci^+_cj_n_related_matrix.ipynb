{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fd6d272-f8a5-4298-a821-1f5313f0bbbe",
      "metadata": {
        "id": "0fd6d272-f8a5-4298-a821-1f5313f0bbbe"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import numpy as np\n",
        "import pickle\n",
        "from joblib import Parallel, delayed\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import pickle\n",
        "from scipy.sparse.linalg import eigsh\n",
        "from scipy import sparse\n",
        "from numpy import genfromtxt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fGtFXEsFitfd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGtFXEsFitfd",
        "outputId": "1fe2d300-6222-41d2-d89b-fde6b8dd17e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyscf\n",
            "  Downloading pyscf-2.10.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: numpy!=1.16,!=1.17,>=1.13 in /usr/local/lib/python3.12/dist-packages (from pyscf) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from pyscf) (1.16.2)\n",
            "Requirement already satisfied: h5py>=2.7 in /usr/local/lib/python3.12/dist-packages (from pyscf) (3.14.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from pyscf) (75.2.0)\n",
            "Downloading pyscf-2.10.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (51.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.2/51.2 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyscf\n",
            "Successfully installed pyscf-2.10.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyscf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FY74rAblijBW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FY74rAblijBW",
        "outputId": "02aa05a2-3cdb-494c-c83a-291a886819ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Check if running in Google Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e656a976-a6fc-4cfa-9fa2-bd7e27ac8e47",
      "metadata": {
        "id": "e656a976-a6fc-4cfa-9fa2-bd7e27ac8e47"
      },
      "outputs": [],
      "source": [
        "# initialize variables\n",
        "# parameters\n",
        "norb = 10\n",
        "N = norb\n",
        "na_true = 5\n",
        "nb_true = na_true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4551297e-863f-4dc5-9390-2d817d90a230",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4551297e-863f-4dc5-9390-2d817d90a230",
        "outputId": "d8b5e12a-62f4-427c-9534-12df6a322482"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of available workers (CPU cores): 8\n"
          ]
        }
      ],
      "source": [
        "import multiprocessing\n",
        "# Get the number of CPU cores\n",
        "num_workers = multiprocessing.cpu_count()\n",
        "print(f\"Number of available workers (CPU cores): {num_workers}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6c1f503-eb2d-4a5a-a8d8-626ae7c5db09",
      "metadata": {
        "id": "f6c1f503-eb2d-4a5a-a8d8-626ae7c5db09"
      },
      "outputs": [],
      "source": [
        "from pyscf.fci import cistring\n",
        "alpha_list = list(cistring.gen_strings4orblist(range(norb), na_true))\n",
        "beta_list  = list(cistring.gen_strings4orblist(range(norb), nb_true))\n",
        "\n",
        "# Build a list of (alpha, beta) pairs in EXACT the same order as PySCF:\n",
        "basis_pair = []\n",
        "for a in alpha_list:\n",
        "    for b in beta_list:\n",
        "        basis_pair.append((a, b))\n",
        "\n",
        "pair_to_index = { ab_pair: i for i, ab_pair in enumerate(basis_pair) }\n",
        "# Now pair_to_index[(alpha_int, beta_int)] = index in PySCF's ordering\n",
        "\n",
        "def D2B(num, W):\n",
        "    # Build a bit array so that array[0] = LSB = orbital 0, array[W-1] = MSB = orbital W-1\n",
        "    # E.g. num=14 -> bin(num)='1110' -> reversed bits => array=[0,1,1,1]\n",
        "    bits_str = bin(num)[2:].rjust(W, '0')  # e.g. \"1110\"\n",
        "    # Reverse so that array[0] is the last digit from bits_str\n",
        "    return np.array(list(bits_str[::-1]), dtype=int)\n",
        "\n",
        "def B2D(array):\n",
        "    # Now array[0] is the LSB, array[-1] is the MSB\n",
        "    res = 0\n",
        "    for i, bit in enumerate(array):\n",
        "        if bit == 1:\n",
        "            res |= (1 << i)\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24fa61af-c40e-41f3-b516-68b772a4b704",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24fa61af-c40e-41f3-b516-68b772a4b704",
        "outputId": "7d18f190-55d2-4a7f-e2c7-28de396e82d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basis dimension is 63504\n"
          ]
        }
      ],
      "source": [
        "print('basis dimension is',len(basis_pair))\n",
        "# print(basis_pair[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f47ebfe5-9818-4e1b-a2b0-f3708adae4de",
      "metadata": {
        "id": "f47ebfe5-9818-4e1b-a2b0-f3708adae4de"
      },
      "outputs": [],
      "source": [
        "# get the potential list for each number operator in product state basis bascailly to check if there's one or 2 eletrons\n",
        "number_op_list = []\n",
        "# the U part\n",
        "for i in range(N): # this is for U term, loop N which is each site\n",
        "    number_occ_list =[]\n",
        "    for index, pair in enumerate(basis_pair):\n",
        "        alpha_array = D2B(pair[0], norb)\n",
        "        beta_array  = D2B(pair[1], norb)\n",
        "         # i is the site index\n",
        "        number_occ = np.int8(alpha_array[i]*beta_array[i])\n",
        "        number_occ_list.append(number_occ) # number of doulbe occ\n",
        "    number_op_list.append(number_occ_list)\n",
        "###########################################################################################\n",
        "\n",
        "# # coulumb interaction but different orbitals , alpha and beta\n",
        "# for i in range(N):\n",
        "#     for j in range(N):\n",
        "#         if j>i:\n",
        "#             number_occ_list =[]\n",
        "#             for index, pair in enumerate(basis_pair):\n",
        "#                 alpha_array = D2B(pair[0], norb)\n",
        "#                 beta_array  = D2B(pair[1], norb)\n",
        "#                 number_occ = np.int8( alpha_array[i]*beta_array[j] + beta_array[i]*alpha_array[j] )\n",
        "#                 number_occ_list.append(number_occ)\n",
        "#             number_op_list.append(number_occ_list)\n",
        "\n",
        "# ###########################################################################################\n",
        "\n",
        "# # exchange interaction different orbitals , same spin\n",
        "# for i in range(N):\n",
        "#     for j in range(N):\n",
        "#         if j>i:\n",
        "#             number_occ_list =[]\n",
        "#             for index, pair in enumerate(basis_pair):\n",
        "#                 alpha_array = D2B(pair[0], norb)\n",
        "#                 beta_array  = D2B(pair[1], norb)\n",
        "#                 number_occ = np.int8( alpha_array[i]*alpha_array[j] + beta_array[i]*beta_array[j] )\n",
        "#                 number_occ_list.append(number_occ)\n",
        "#             number_op_list.append(number_occ_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e46eaf84-8b7b-4391-999f-95df7033e0dc",
      "metadata": {
        "id": "e46eaf84-8b7b-4391-999f-95df7033e0dc"
      },
      "outputs": [],
      "source": [
        "number_op_list = np.array(number_op_list)\n",
        "\n",
        "\n",
        "string_name = f'number_op_list_norb={N}_na_{na_true}_nb_{nb_true}.txt'\n",
        "if IN_COLAB:\n",
        "    file_path = f'/content/drive/My Drive/Quantum_chemistry/hardware_effient/{string_name}'\n",
        "else:\n",
        "    file_path = string_name\n",
        "\n",
        "np.savetxt(file_path, number_op_list,fmt='%d')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c29d828-fd32-49e7-8a41-3071deadbb65",
      "metadata": {
        "id": "6c29d828-fd32-49e7-8a41-3071deadbb65"
      },
      "outputs": [],
      "source": [
        "# Initialize the dictionary for hopping pairs\n",
        "nn_condition = False  # True for nearest-neighbor hopping\n",
        "hopping_cos_dict = {}\n",
        "for j in range(N):\n",
        "    for k in range(N):\n",
        "        if nn_condition:\n",
        "            if j == k+1:\n",
        "                hopping_cos_dict[(j, k, 1)] = []  # Key for positive spin\n",
        "                hopping_cos_dict[(j, k, -1)] = []  # Key for negative spin\n",
        "        else:\n",
        "            if k<j:\n",
        "                # Include keys for both positive and negative spins\n",
        "                hopping_cos_dict[(j, k, 1)] = []  # Key for positive spin\n",
        "                hopping_cos_dict[(j, k, -1)] = []  # Key for negative spin\n",
        "\n",
        "# Populate the dictionary\n",
        "for (j, k, spin) in hopping_cos_dict.keys():\n",
        "    good_index_list = []\n",
        "    for index, pair in enumerate(basis_pair):\n",
        "        alpha_array = D2B(pair[0], norb)\n",
        "        beta_array  = D2B(pair[1], norb)\n",
        "        if spin == 1:\n",
        "            value = alpha_array[j] + alpha_array[k]\n",
        "        else:\n",
        "            value = beta_array[j] + beta_array[k]\n",
        "        # here i dont care if it is i to j or j to i, bc it will be hermitian when implemented later in teh UCC_t_part code\n",
        "        if value == 1:\n",
        "            good_index_list.append(np.int64(index))\n",
        "            # THIS INT64 CAN NOT BE CHANGED, IT WILL BE USED AS INDEX IN THE FUTURE\n",
        "            ################################################################################\n",
        "            ################################################################################\n",
        "    # Update the dictionary\n",
        "    hopping_cos_dict[(j, k, spin)] = np.array(good_index_list)\n",
        "\n",
        "# Now hopping_cos_dict contains the good index lists for all (i, j, spin) pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dad58893-e97e-461a-805a-5235ff9be6f7",
      "metadata": {
        "id": "dad58893-e97e-461a-805a-5235ff9be6f7"
      },
      "outputs": [],
      "source": [
        "string_name = f'torch_hopping_cose_dict_norb={N}_na_{na_true}_nb_{nb_true}.pkl'\n",
        "if IN_COLAB:\n",
        "    file_path = f'/content/drive/My Drive/Quantum_chemistry/hardware_effient/{string_name}'\n",
        "else:\n",
        "    file_path = string_name\n",
        "\n",
        "with open(file_path, 'wb') as f:\n",
        "    pickle.dump(hopping_cos_dict, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d5c1817-0adb-4dfe-9a63-d9d080f8fdfc",
      "metadata": {
        "id": "9d5c1817-0adb-4dfe-9a63-d9d080f8fdfc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2a3ece4-d0e0-44bd-9758-2ee06f97ec2a",
      "metadata": {
        "id": "d2a3ece4-d0e0-44bd-9758-2ee06f97ec2a"
      },
      "outputs": [],
      "source": [
        "# Initialize the dictionary for hopping pairs\n",
        "hopping_dict = {}\n",
        "for j in range(N):\n",
        "    for k in range(N):\n",
        "        if nn_condition:\n",
        "            if j == k+1:\n",
        "                hopping_dict[(j, k, 1)] = {'row': [], 'col': [], 'data': []}\n",
        "                hopping_dict[(j, k, -1)] = {'row': [], 'col': [], 'data': []}\n",
        "        else:\n",
        "            if k<j:\n",
        "                hopping_dict[(j, k, 1)] = {'row': [], 'col': [], 'data': []}  # Key for positive spin\n",
        "                hopping_dict[(j, k, -1)] = {'row': [], 'col': [], 'data': []}  # Key for negative spin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d7168e6-3749-4f84-a6f3-097a41e90728",
      "metadata": {
        "id": "7d7168e6-3749-4f84-a6f3-097a41e90728"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "224c0a64-e4b8-4120-8627-d1350940ce44",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "224c0a64-e4b8-4120-8627-d1350940ce44",
        "outputId": "0c00a8e7-dab0-49ff-b05d-8196f9ee4888"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Chunk 0] Processing indices 0 to 50000...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Chunk 0: 100%|██████████| 50000/50000 [18:37<00:00, 44.73it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Chunk 0] Saved at /content/drive/My Drive/Quantum_chemistry/hardware_effient/op_pieces/hopping_chunk_0.pkl\n",
            "[Chunk 1] Processing indices 50000 to 63504...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Chunk 1: 100%|██████████| 13504/13504 [14:01<00:00, 16.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Chunk 1] Saved at /content/drive/My Drive/Quantum_chemistry/hardware_effient/op_pieces/hopping_chunk_1.pkl\n",
            "Found 2/2 chunk files.\n",
            "All chunk files present. Merging into final hopping_dict...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Merging chunks: 100%|██████████| 2/2 [00:01<00:00,  1.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final hopping_dict saved at: /content/drive/My Drive/Quantum_chemistry/hardware_effient/op_pieces/torch_sparse_matrices_dict_norb=10_na_5_nb_5.pkl\n"
          ]
        }
      ],
      "source": [
        "#############################\n",
        "# 2) FUNCTION FOR ONE INDEX #\n",
        "#############################\n",
        "\n",
        "def process_single_index(index):\n",
        "    \"\"\"\n",
        "    Compute the hopping contributions for a single basis-pair index.\n",
        "    Returns a dictionary with keys (j_loc, k_loc, spin_sign),\n",
        "    each value is a dict containing 'row', 'col', 'data' lists.\n",
        "    \"\"\"\n",
        "    local_dict = {}\n",
        "\n",
        "    pair = basis_pair[index]      # pair is (alphaDetInt, betaDetInt)\n",
        "    alpha_array = D2B(pair[0], norb)\n",
        "    beta_array  = D2B(pair[1], norb)\n",
        "\n",
        "    for j_loc in range(N):       # \"destruction\" index\n",
        "        for k_loc in range(N):   # \"creation\" index\n",
        "            # Condition checks\n",
        "            if nn_condition:\n",
        "                # Only allow j_loc == k_loc + 1\n",
        "                if j_loc != k_loc + 1:\n",
        "                    continue\n",
        "            else:\n",
        "                # Only allow k_loc < j_loc\n",
        "                if k_loc >= j_loc:\n",
        "                    continue\n",
        "\n",
        "            # For each spin sign\n",
        "            for spin_sgn in [1, -1]:\n",
        "                state_array = alpha_array if spin_sgn == 1 else beta_array\n",
        "\n",
        "                # Check if we can \"hop\" from j_loc to k_loc\n",
        "                if state_array[j_loc] == 1 and state_array[k_loc] == 0:\n",
        "                    old_det_int  = B2D(state_array)\n",
        "                    phase_factor = cistring.cre_des_sign(k_loc, j_loc, old_det_int)\n",
        "\n",
        "                    # Hop the electron\n",
        "                    new_state_array = state_array.copy()\n",
        "                    new_state_array[j_loc] = 0\n",
        "                    new_state_array[k_loc] = 1\n",
        "\n",
        "                    # Build the new tuple\n",
        "                    if spin_sgn == 1:\n",
        "                        new_tuple = (B2D(new_state_array), B2D(beta_array))\n",
        "                    else:\n",
        "                        new_tuple = (B2D(alpha_array), B2D(new_state_array))\n",
        "\n",
        "                    # Get index of new_tuple in basis_pair\n",
        "                    # If your basis_pair is huge, consider a dict mapping {tuple: idx} for speed\n",
        "                    col_index = basis_pair.index(new_tuple)\n",
        "\n",
        "                    key = (j_loc, k_loc, spin_sgn)\n",
        "                    if key not in local_dict:\n",
        "                        local_dict[key] = {'row': [], 'col': [], 'data': []}\n",
        "                    # need to change int data type if working with more than 16 spatial orbitals\n",
        "                    ################################################################################\n",
        "                    ################################################################################\n",
        "                    local_dict[key]['row'].append(np.int64(index))\n",
        "                    local_dict[key]['col'].append(np.int64(col_index))\n",
        "                    local_dict[key]['data'].append(np.int8(phase_factor))\n",
        "                    ################################################################################\n",
        "    return local_dict\n",
        "\n",
        "######################################\n",
        "# 3) CHUNKING PARAMETERS AND FOLDERS #\n",
        "######################################\n",
        "\n",
        "num_cores = 16               # Number of CPU cores to use\n",
        "total_dets = len(basis_pair)   # Total number of basis-pair states\n",
        "chunk_size = 50000            # Adjust based on memory/time constraints\n",
        "num_chunks = (total_dets + chunk_size - 1) // chunk_size\n",
        "\n",
        "\n",
        "if IN_COLAB:\n",
        "    save_folder = f'/content/drive/My Drive/Quantum_chemistry/hardware_effient/op_pieces'\n",
        "else:\n",
        "    save_folder = os.path.join(os.getcwd(), \"op_pieces\")\n",
        "os.makedirs(save_folder, exist_ok=True)\n",
        "\n",
        "##########################\n",
        "# 4) PROCESS EACH CHUNK  #\n",
        "##########################\n",
        "\n",
        "def process_chunk(chunk_idx):\n",
        "    \"\"\"\n",
        "    Process the chunk of indices for chunk_idx,\n",
        "    and save the partial hopping dictionary to a pickle file.\n",
        "    \"\"\"\n",
        "    import pickle  # local import to avoid issues in parallel\n",
        "    from tqdm import tqdm  # local import for progress bar\n",
        "\n",
        "    chunk_file = os.path.join(save_folder, f\"hopping_chunk_{chunk_idx}.pkl\")\n",
        "    if os.path.exists(chunk_file):\n",
        "        print(f\"[Chunk {chunk_idx}] Already exists. Skipping.\")\n",
        "        return  # Skip re-computation\n",
        "\n",
        "    start_idx = chunk_idx * chunk_size\n",
        "    end_idx = min((chunk_idx + 1) * chunk_size, total_dets)\n",
        "    print(f\"[Chunk {chunk_idx}] Processing indices {start_idx} to {end_idx}...\")\n",
        "\n",
        "    # Parallel compute local dictionaries for each index in [start_idx, end_idx)\n",
        "    # We attach a progress bar *outside* the parallel function to reduce pickling issues.\n",
        "    results = []\n",
        "    for i in tqdm(range(start_idx, end_idx), desc=f\"Chunk {chunk_idx}\"):\n",
        "        # Just call process_single_index directly in a loop\n",
        "        # If you want parallel calls, see below for how to do it carefully\n",
        "        results.append(process_single_index(i))\n",
        "\n",
        "    # If you prefer parallel calls, you can do:\n",
        "    # results = Parallel(n_jobs=num_cores)(\n",
        "    #     delayed(process_single_index)(i) for i in range(start_idx, end_idx)\n",
        "    # )\n",
        "    # but you might need to remove the tqdm or wrap it carefully.\n",
        "\n",
        "    # Merge these local dicts into one chunk dict\n",
        "    chunk_hopping_dict = {}\n",
        "    for local_res in results:\n",
        "        for key, val in local_res.items():\n",
        "            if key not in chunk_hopping_dict:\n",
        "                chunk_hopping_dict[key] = {'row': [], 'col': [], 'data': []}\n",
        "            chunk_hopping_dict[key]['row'].extend(val['row'])\n",
        "            chunk_hopping_dict[key]['col'].extend(val['col'])\n",
        "            chunk_hopping_dict[key]['data'].extend(val['data'])\n",
        "\n",
        "    # Save the chunk dictionary to disk\n",
        "    with open(chunk_file, 'wb') as f:\n",
        "        pickle.dump(chunk_hopping_dict, f)\n",
        "\n",
        "    print(f\"[Chunk {chunk_idx}] Saved at {chunk_file}\")\n",
        "\n",
        "###################\n",
        "# 5) RUN ALL CHUNKS\n",
        "###################\n",
        "for chunk_idx in range(num_chunks):\n",
        "    process_chunk(chunk_idx)\n",
        "\n",
        "##############################\n",
        "# 6) CHECK WHICH CHUNKS EXIST\n",
        "##############################\n",
        "chunk_files = []\n",
        "for i in range(num_chunks):\n",
        "    fname = os.path.join(save_folder, f\"hopping_chunk_{i}.pkl\")\n",
        "    if os.path.exists(fname):\n",
        "        chunk_files.append(fname)\n",
        "\n",
        "print(f\"Found {len(chunk_files)}/{num_chunks} chunk files.\")\n",
        "\n",
        "#########################################\n",
        "# 7) MERGE CHUNKS IF ALL ARE AVAILABLE  #\n",
        "#########################################\n",
        "if len(chunk_files) == num_chunks:\n",
        "    print(\"All chunk files present. Merging into final hopping_dict...\")\n",
        "\n",
        "    hopping_dict = {}\n",
        "    for cf in tqdm(chunk_files, desc=\"Merging chunks\"):\n",
        "        with open(cf, 'rb') as f:\n",
        "            partial_dict = pickle.load(f)\n",
        "\n",
        "        # Merge partial_dict into global hopping_dict\n",
        "        for key, val in partial_dict.items():\n",
        "            if key not in hopping_dict:\n",
        "                hopping_dict[key] = {'row': [], 'col': [], 'data': []}\n",
        "            hopping_dict[key]['row'].extend(val['row'])\n",
        "            hopping_dict[key]['col'].extend(val['col'])\n",
        "            hopping_dict[key]['data'].extend(val['data'])\n",
        "\n",
        "    # Convert each entry in hopping_dict to a sparse CSR matrix\n",
        "    for key in hopping_dict:\n",
        "        row = np.array(hopping_dict[key]['row'])\n",
        "        col = np.array(hopping_dict[key]['col'])\n",
        "        data = np.array(hopping_dict[key]['data'])\n",
        "        hopping_dict[key] = sparse.csr_matrix(\n",
        "            (data, (row, col)), shape=(total_dets, total_dets)\n",
        "        )\n",
        "\n",
        "    # Save final dictionary\n",
        "    final_file = os.path.join(save_folder, f\"torch_sparse_matrices_dict_norb={norb}_na_{na_true}_nb_{nb_true}.pkl\")\n",
        "    with open(final_file, 'wb') as f:\n",
        "        pickle.dump(hopping_dict, f)\n",
        "\n",
        "    print(f\"Final hopping_dict saved at: {final_file}\")\n",
        "\n",
        "    # Optionally remove the chunk files\n",
        "    # for cf in chunk_files:\n",
        "    #     os.remove(cf)\n",
        "    # print(\"All chunk files removed after merging.\")\n",
        "\n",
        "else:\n",
        "    print(\"Not all chunks are present. Rerun or check partial runs.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df9dcffe-e3e7-4fe8-9ba4-5bb57234bd54",
      "metadata": {
        "id": "df9dcffe-e3e7-4fe8-9ba4-5bb57234bd54"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}